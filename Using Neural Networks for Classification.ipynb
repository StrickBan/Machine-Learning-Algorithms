{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Neural Networks for Classification on MNIST and Iris Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using Neural Networks with Softmax in the last layer on classification problems: MNIST and Iris datasets. First I will solve them \"by hand\" (not using Neural Networks's implementations on any library). Later, I will solve the problems using the Scikit library and compare the results obtained from my \"by hand\" solution with the results obtained from the solution using the Scikit library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score # \n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import numpy.random as r\n",
    "import matplotlib.pyplot as plt \n",
    "import timeit\n",
    "\n",
    "def convert_y_to_vect(y, outputs):\n",
    "    y_vect = np.zeros((len(y), outputs))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect\n",
    "\n",
    "def f(z): \n",
    "#sigmoid\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "\n",
    "def f_deriv(z):\n",
    "#sigmoid\n",
    "    return f(z) * (1 - f(z))\n",
    "\n",
    "def softmax(z):\n",
    "    sum=0\n",
    "\n",
    "    for i in range(len(z)):\n",
    "        sum = sum  + np.exp(z[i])\n",
    "    z=np.exp(z)/sum\n",
    "\n",
    "    return z\n",
    "\n",
    "def softmax_deriv(z):\n",
    "    return softmax(z)*(1-softmax(z))\n",
    "\n",
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} \n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1]))\n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b\n",
    "\n",
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b\n",
    "\n",
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "        a[l+1] = f(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "\n",
    "    return a, z\n",
    "\n",
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-a_out) * softmax_deriv(z_out) \n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)\n",
    "\n",
    "\n",
    "def train_nn(nn_structure, X, y, lamb, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                    \n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            #Question a\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l] +  lamb * W[l])\n",
    "            #W[l] += -alpha * (1.0/N * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 3000 iterations\n",
      "Iteration 0 of 3000\n",
      "Iteration 1000 of 3000\n",
      "Iteration 2000 of 3000\n",
      "Time:  235.37646590000077\n",
      "Prediction accuracy is 94.1585535465925%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits \n",
    "\n",
    "# Load data\n",
    "digits=load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Scale data\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)\n",
    "\n",
    "# Split the data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# convert digits to vectors\n",
    "y_v_train = convert_y_to_vect(y_train, 10)\n",
    "y_v_test = convert_y_to_vect(y_test, 10)\n",
    "\n",
    "# Neural Network\n",
    "nn_structure = [64, 30, 10]\n",
    "    \n",
    "# train the NN\n",
    "start = timeit.default_timer()\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 0.001, 3000, 0.25)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)\n",
    "\n",
    "# get the prediction accuracy and print\n",
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 3000 iterations\n",
      "Iteration 0 of 3000\n",
      "Iteration 1000 of 3000\n",
      "Iteration 2000 of 3000\n",
      "Time:  18.313312599999335\n",
      "Prediction accuracy is 97.36842105263158%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Reading data\n",
    "df = pd.read_csv('datasets/iris.csv', header=None, names = [\"sepal length[cm]\",\"sepal width[cm]\",\"petal length[cm]\", \"petal width\", \"label\"])\n",
    "df['label'] = df.label.map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
    "\n",
    "# Splitting data\n",
    "names = [\"sepal length[cm]\",\"sepal width[cm]\",\"petal length[cm]\", \"petal width\"]\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df[names],df['label'], random_state=0)\n",
    "\n",
    "X_train=df_X_train.to_numpy()\n",
    "X_test=df_X_test.to_numpy()\n",
    "y_train=df_y_train.to_numpy()\n",
    "y_test=df_y_test.to_numpy()\n",
    "\n",
    "# Scaling data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# converting to vectors\n",
    "y_v_train = convert_y_to_vect(y_train, 3)\n",
    "y_v_test = convert_y_to_vect(y_test, 3)\n",
    "\n",
    "nn_structure = [4, 3, 3]\n",
    "    \n",
    "# training the NN\n",
    "start = timeit.default_timer()\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 0.001, 3000, 0.25)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)\n",
    "\n",
    "# getting the prediction accuracy and printing\n",
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.77468706536857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "import numpy.random as r\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Loading data\n",
    "digits=load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Scaling data\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)\n",
    "\n",
    "# Split the data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Function to convert y to vector\n",
    "def convert_y_to_vect(y, outputs):\n",
    "    y_vect = np.zeros((len(y), outputs))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect\n",
    "\n",
    "# convert to vectors\n",
    "y_v_train = convert_y_to_vect(y_train, 10)\n",
    "y_v_test = convert_y_to_vect(y_test, 10)\n",
    "\n",
    "# Running the algorithm\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', alpha=1e-5, hidden_layer_sizes=(30,), random_state=1, learning_rate_init=0.5)\n",
    "clf.fit(X_train, y_v_train)\n",
    "clf.out_activation_ = 'softmax'\n",
    "\n",
    "# Predicting on test data\n",
    "score = clf.score(X_test, y_v_test)\n",
    "print(\"Accuracy:\", score * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.36842105263158\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Reading data\n",
    "df = pd.read_csv('datasets/iris.csv', header=None, names = [\"sepal length[cm]\",\"sepal width[cm]\",\"petal length[cm]\", \"petal width\", \"label\"])\n",
    "df['label'] = df.label.map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
    "\n",
    "# Splitting data\n",
    "names = [\"sepal length[cm]\",\"sepal width[cm]\",\"petal length[cm]\", \"petal width\"]\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df[names],df['label'], random_state=0)\n",
    "\n",
    "X_train=df_X_train.to_numpy()\n",
    "X_test=df_X_test.to_numpy()\n",
    "y_train=df_y_train.to_numpy()\n",
    "y_test=df_y_test.to_numpy()\n",
    "\n",
    "# Scaling data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# convert to vectors\n",
    "y_v_train = convert_y_to_vect(y_train, 3)\n",
    "y_v_test = convert_y_to_vect(y_test, 3)\n",
    "\n",
    "# Running the algorithm\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5,activation='logistic', hidden_layer_sizes=(3,), random_state=1, learning_rate_init=2)\n",
    "clf.fit(X_train, y_v_train)\n",
    "clf.out_activation_ = 'softmax'\n",
    "\n",
    "# Predicting on test data\n",
    "score = clf.score(X_test, y_v_test)\n",
    "print(\"Accuracy:\", score * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained were the same for the Iris dataset (97.36% accuracy). However, the Scikit implementation performed better than the \"by hand\" implementation for the MNIST dataset (97.77% vs 94.15% accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
