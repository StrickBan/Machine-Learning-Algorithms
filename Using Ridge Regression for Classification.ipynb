{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Ridge Regression for Classification on Breast Cancer and Wine Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using Ridge Regression on classification problems: Breast Cancer and Wine datasets. First I will solve them \"by hand\" (not using Ridge Regression's implementations on any library). Later, I will solve the problems using the Scikit library and compare the results obtained from my \"by hand\" solution with the results obtained from the solution using the Scikit library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer # taking included data set from Sklearn http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "from sklearn import preprocessing # preprossing is what we do with the data before we run the learning algorithm\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "        \n",
    "    if (isinstance(z, int)):\n",
    "        sigmoid = 1/(1 + np.exp((-1)*z))\n",
    "    else:\n",
    "        sigmoid = np.zeros(((z.shape[0]),1))\n",
    "        for i in range(z.shape[0]):\n",
    "            sigmoid[i][0] = 1/(1 + np.exp((-1)*z[i]))\n",
    "    return sigmoid\n",
    "\n",
    "\n",
    "# Hypothesis function\n",
    "def hypothesis(X , w):\n",
    "\n",
    "    h = np.zeros(((X.shape[0]),(w.shape[1])))\n",
    "    for i in range (X.shape[0]):\n",
    "        value = 0\n",
    "        for j in range (X.shape[1]):\n",
    "            value = value + X[i][j] * w[j]\n",
    "        h[i] = value\n",
    "    hfinal = np.zeros((h.shape))\n",
    "    hfinal = sigmoid(h)\n",
    "    return hfinal\n",
    "    \n",
    "# Log likelihood function now with lambda hyperparameter\n",
    "def log_likelihood(X , y , w, lamb):\n",
    "    \n",
    "    h = hypothesis(X, w)\n",
    "    log_likelihood = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        log_likelihood = log_likelihood + (y[i]* np.log(h[i]) + (1-y[i])*np.log(1-h[i])) \n",
    "    \n",
    "    reg = lamb*(np.dot(np.transpose(w),w))\n",
    "\n",
    "    return (log_likelihood - reg)\n",
    "\n",
    "# Gradient descent function\n",
    "def Logistic_Regresion_Gradient_Ascent(X, y, learning_rate, num_iters, lamb):\n",
    "    # For every 100 iterations, store the log_likelihood for the current w\n",
    "    # Initializing log_likelihood to be an empty list  \n",
    "    log_likelihood_values = []\n",
    "    # Initialize w to be a zero vector of shape x_train.shape[1],1\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    # Initialize N to the number of training examples\n",
    "    N = X.shape[0] \n",
    "    \n",
    "    for i in range (0, num_iters):\n",
    "        h = hypothesis(X, w)\n",
    "        \n",
    "        for j in range (w.shape[0]):\n",
    "            sum = 0\n",
    "            for k in range (0, N):\n",
    "                 sum = sum + (y[k]-h[k])*X[k][j]\n",
    "            w[j] = w[j] + sum*learning_rate/N\n",
    "        \n",
    "        if (i % 100) == 0:\n",
    "            log_likelihood_values.append(log_likelihood(X,y,w, lamb))\n",
    "        \n",
    "    return w, log_likelihood_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.2027972027972\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "cancer = load_breast_cancer()\n",
    "y = cancer.target\n",
    "X = cancer.data\n",
    "\n",
    "# train_test_split to split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Append a column of ones to x_train \n",
    "ones = np.ones(X_train.shape[0]).reshape((X_train.shape[0], 1))\n",
    "X_train = np.hstack((ones, X_train))\n",
    "\n",
    "# Append a column of ones to x_test\n",
    "ones = np.ones(X_test.shape[0]).reshape((X_test.shape[0], 1))\n",
    "X_test = np.hstack((ones, X_test))\n",
    "\n",
    "# Initialize parameters w\n",
    "w = np.zeros((X_train.shape[1], 1))\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.5\n",
    "num_iters = 5000\n",
    "lamb = 10\n",
    "\n",
    "w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters, lamb)\n",
    "\n",
    "# Evaluating the model\n",
    "h = np.zeros(((X_test.shape[0]),(w.shape[1])))\n",
    "h = hypothesis(X_test , w)\n",
    "\n",
    "# Predicting on test data\n",
    "predicted = np.zeros(((h.shape[0]),1))\n",
    "\n",
    "for i in range (h.shape[0]):\n",
    "    if h[i] >= 0.50:\n",
    "        predicted[i] = 1\n",
    "    else:\n",
    "        predicted[i] = 0\n",
    "correct = 0\n",
    "\n",
    "for j in range (predicted.shape[0]):\n",
    "    if predicted[j] == 1 and y_test[j] == 1 or predicted[j] == 0 and y_test[j] == 0:\n",
    "        correct+=1\n",
    "\n",
    "\n",
    "# Reporting accuracy\n",
    "accuracy = correct/h.shape[0]\n",
    "\n",
    "print(\"Accuracy:\", accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.96969696969697\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "# Loading the dataset\n",
    "data = genfromtxt('datasets/wine.csv', delimiter=',') # reading in the data matrix\n",
    "\n",
    "# data preprocessing (Only using 2 classes of wine)\n",
    "n =  data.shape[0]\n",
    "feat = data.shape[1]\n",
    "data = data[data[:,0]<3]\n",
    "X = data[0:n,1:feat]\n",
    "y = data[0:n,0]\n",
    "y = np.where(y<2,0,1)\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)\n",
    "\n",
    "# Scaling Data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Append a column of ones to x_train \n",
    "ones = np.ones(X_train.shape[0]).reshape((X_train.shape[0], 1))\n",
    "X_train = np.hstack((ones, X_train))\n",
    "\n",
    "# Append a column of ones to x_test\n",
    "ones = np.ones(X_test.shape[0]).reshape((X_test.shape[0], 1))\n",
    "X_test = np.hstack((ones, X_test))\n",
    "\n",
    "# Initialize parameters w\n",
    "w = np.zeros((X_train.shape[1], 1))\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.5\n",
    "num_iters = 5000\n",
    "lamb = 10\n",
    "\n",
    "w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters, lamb)\n",
    "\n",
    "# Evaluating the model\n",
    "h = np.zeros(((X_test.shape[0]),(w.shape[1])))\n",
    "h = hypothesis(X_test , w)\n",
    "\n",
    "# Predicting on test data\n",
    "predicted = np.zeros(((h.shape[0]),1))\n",
    "\n",
    "for i in range (h.shape[0]):\n",
    "    if h[i] >= 0.50:\n",
    "        predicted[i] = 1\n",
    "    else:\n",
    "        predicted[i] = 0\n",
    "correct = 0\n",
    "\n",
    "for j in range (predicted.shape[0]):\n",
    "    if predicted[j] == 1 and y_test[j] == 1 or predicted[j] == 0 and y_test[j] == 0:\n",
    "        correct+=1\n",
    "\n",
    "# Reporting accuracy\n",
    "accuracy = correct/h.shape[0]\n",
    "\n",
    "print(\"Accuracy:\", accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.2027972027972\n"
     ]
    }
   ],
   "source": [
    "# Importing important libraries\n",
    "from sklearn.datasets import load_breast_cancer # taking included data set from Sklearn http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Loading the dataset\n",
    "cancer = load_breast_cancer()\n",
    "y = cancer.target\n",
    "X = cancer.data\n",
    "\n",
    "# train_test_split to split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)\n",
    "\n",
    "# Scaling data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_reg = SGDClassifier(loss='log', alpha=0.5, max_iter=5000, shuffle=False )\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "score = ridge_reg.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.96969696969697\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('datasets/wine.csv', delimiter=',') # reading in the data matrix\n",
    "\n",
    "# data preprocessing (Only using 2 classes of wine)\n",
    "n =  data.shape[0]\n",
    "feat = data.shape[1]\n",
    "data = data[data[:,0]<3]\n",
    "X = data[0:n,1:feat]\n",
    "y = data[0:n,0]\n",
    "y = np.where(y<2,0,1)\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)\n",
    "\n",
    "# Scaling Data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Ridge regression\n",
    "ridge_reg = SGDClassifier(loss='log', alpha=0.5, max_iter=5000, shuffle=False )\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "score = ridge_reg.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtained the same accuracies with both implementations (\"by hand\" and Scikit) on both datasets (Breast Cancer and Wine datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
